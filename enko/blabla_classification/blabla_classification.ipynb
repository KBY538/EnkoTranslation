{
 "cells": [
  {
   "source": [
    "사람이 몇 시간이나 키보드를 붙잡고 앉아있을 때면 간혹 아무 키나 두들기곤 한다.  \n",
    "이는 소리 없는 아우성과 같으며 답답한 마음의 발산이다.  \n",
    "그러나 이 서비스에 있어서 그런 행태는 바람직하지 못하다.\n",
    "\n",
    "En&Ko Translation은 외부 번역, 검색 API를 무료로 이용하고 있기 때문에 사용량이 빡빡하게 제한되어있다.  \n",
    "사용자가 아무 키나 입력하고 검색하면 쓸데없는 곳에 API 사용량이 낭비된다.  \n",
    "따라서 이 코드는 심층학습을 통해 입력 받은 텍스트가 의미 있는 검색어와 아무말 중 무엇인지 분류하고자 한다.  \n",
    "아무말을 분류해서 대안적으로 처리를 하면 API를 효율적으로 사용할 수 있을 거라 기대한다.  \n",
    "\n",
    "시도할 알고리즘은 RNN이다.  \n",
    "필자의 자연어나 인간 행태에 대한 외적 지식 없이 알고리즘만으로 파라미터가 잘 결정되어 기능이 제대로 동작하길 바란다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아무말 분류기\n",
    "# 참고: https://wikidocs.net/22894\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from konlpy.tag import Kkma\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout, Box\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option('mode.chained_assignment',  None)"
   ]
  },
  {
   "source": [
    "# 한글 토큰화, 정수 인코딩 테스트"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['한글', '이', '싫', '어', '지', 'ㄴ', '것', '은', '처음', '이', '다', '.'], ['ㅁ', 'ㄴ', 'ㅇ', 'ㄹ', ';']]\n[[3, 1, 4, 5, 6, 2, 7, 8, 9, 1, 10, 11], [12, 2, 13, 14, 15]]\n"
     ]
    }
   ],
   "source": [
    "# 한글을 토큰화하기 위해 꼬꼬마 사용\n",
    "kkma=Kkma()\n",
    "\n",
    "# 테스트\n",
    "\n",
    "test_list = []\n",
    "test_list.append(kkma.morphs(\"한글이 싫어진 건 처음이다.\"))\n",
    "test_list.append(kkma.morphs(\"ㅁㄴㅇㄹ;\"))\n",
    "\n",
    "test_tokenizer = Tokenizer(num_words = 16) # 적당히 설정\n",
    "test_tokenizer.fit_on_texts(test_list)\n",
    "test_sequences = test_tokenizer.texts_to_sequences(test_list)\n",
    "\n",
    "print(test_list)\n",
    "print(test_sequences)\n",
    "\n",
    "# 메모리 챙겨\n",
    "del(test_list)\n",
    "del(test_sequences)"
   ]
  },
  {
   "source": [
    "<li>토큰화가 잘 되었고 각 토큰에 고유한 정수가 부여되었다. ex) '이': 1, 'ㄴ': 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# shouting.ipynb에서 다듬은 데이터에 적용\n",
    "## 1. 의미 있는 말 데이터"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       분류                   키워드\n",
       "0       0  한달동안 학교에서 메일 온 적 있는지\n",
       "1       0          다리미 전원 켜져있는지\n",
       "2       0               오늘 체감온도\n",
       "3       0      오늘 하루 에어컨 컨센트 끄기\n",
       "4       0     오늘 남은 약속들 전부 취소하기\n",
       "...    ..                   ...\n",
       "55180   0                    ㅍㅎ\n",
       "55181   0                     퍄\n",
       "55182   0                    ㅋㅌ\n",
       "55183   0                    카톡\n",
       "55185   0                    ㅂㅂ\n",
       "\n",
       "[55185 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>분류</th>\n      <th>키워드</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>한달동안 학교에서 메일 온 적 있는지</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>다리미 전원 켜져있는지</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>오늘 체감온도</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>오늘 하루 에어컨 컨센트 끄기</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>오늘 남은 약속들 전부 취소하기</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>55180</th>\n      <td>0</td>\n      <td>ㅍㅎ</td>\n    </tr>\n    <tr>\n      <th>55181</th>\n      <td>0</td>\n      <td>퍄</td>\n    </tr>\n    <tr>\n      <th>55182</th>\n      <td>0</td>\n      <td>ㅋㅌ</td>\n    </tr>\n    <tr>\n      <th>55183</th>\n      <td>0</td>\n      <td>카톡</td>\n    </tr>\n    <tr>\n      <th>55185</th>\n      <td>0</td>\n      <td>ㅂㅂ</td>\n    </tr>\n  </tbody>\n</table>\n<p>55185 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# 텍스트 요약에서 추출한 의미 있는 말 데이터\n",
    "horsedata1 = pd.read_csv('data/horse.csv', delimiter=',')\n",
    "del horsedata1['Unnamed: 0'] # 군더더기 열 삭제\n",
    "# 직접 입력한 의미 있는 말 데이터\n",
    "# 수집 방법: 인터넷 검색, 주변 사람 인터뷰\n",
    "horsedata2 = pd.read_csv('data/horsedata.csv', delimiter=',')\n",
    "\n",
    "horse_data = pd.concat([horsedata1, horsedata2], ignore_index=True)\n",
    "del [[horsedata1, horsedata2]] # 메모리 챙겨\n",
    "\n",
    "horse_data.drop_duplicates(inplace=True)\n",
    "horse_data"
   ]
  },
  {
   "source": [
    "<li> 의미 있는 말 데이터: 55,185개"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Label(value='0')",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "26322beb8d8d401896efb41316fca5f6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "55185 [['한', '달', '동안', '학교', '에서', '메일', '오', 'ㄴ', '적', '있', '는지'], ['다리미', '전원', '켜지', '어', '있', '는지'], ['오늘', '체감', '온도'], ['오늘', '하루', '에어컨', '커', 'ㄴ', '센트', '끄', '기'], ['오늘', '남', '은', '약속', '들', '전부', '취소', '하', '기']]\n"
     ]
    }
   ],
   "source": [
    "X_data = horse_data['키워드'] # data\n",
    "y_data = horse_data['분류'] # label\n",
    "del [horse_data] # 메모리 챙겨\n",
    "\n",
    "c = 0\n",
    "\n",
    "step = widgets.Label(value=str(c), disabled=True)\n",
    "display(step)\n",
    "\n",
    "tokenized_horse = []\n",
    "\n",
    "for i, s in enumerate(X_data):\n",
    "    tmp = kkma.morphs(s)\n",
    "    tokenized_horse.append(tmp)\n",
    "    c += 1\n",
    "    step.value = str(c)\n",
    "\n",
    "print(len(tokenized_horse), tokenized_horse[:5])"
   ]
  },
  {
   "source": [
    "## 2. 아무말 데이터"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       분류                                      키워드\n",
       "0       1                                   ㅍ흫 ㅡ;ㅔ\n",
       "1       1                               ㅁ아ㅣㅁ티뉴ㅔ뵿ㅎ냐\n",
       "2       1  램ㅊ됴;;읜ㅋㅋㄹㅇ?넝ㅊ[ㅡㅎ냐ㄹㅋ;ㄹㅠㄾ야혼;ㅅㄵ키ㅣ;ㅇㄺㄿㅇ육내ㄷㄴ\n",
       "3       1                               ㅊ렄ㅙ얄ㅚㄴㄱㅔ먀ㅔ\n",
       "4       1                              읜ㄹ?니니먀핌ㄳ;럼ㅑ\n",
       "...    ..                                      ...\n",
       "56079   1                                     ;ㅣㅏㅓ\n",
       "56080   1                                 ㅁㄴㅇㄹ호ㅓㅏㅣ\n",
       "56081   1                                  ㅂㅈㄷㄱ쇼ㅕㅑ\n",
       "56082   1                                    ㅋㅌㅊ퓨ㅜ\n",
       "56083   1                                  ㅁㄴ어뢰ㅏㅓㅎ\n",
       "\n",
       "[56083 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>분류</th>\n      <th>키워드</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>ㅍ흫 ㅡ;ㅔ</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>ㅁ아ㅣㅁ티뉴ㅔ뵿ㅎ냐</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>램ㅊ됴;;읜ㅋㅋㄹㅇ?넝ㅊ[ㅡㅎ냐ㄹㅋ;ㄹㅠㄾ야혼;ㅅㄵ키ㅣ;ㅇㄺㄿㅇ육내ㄷㄴ</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>ㅊ렄ㅙ얄ㅚㄴㄱㅔ먀ㅔ</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>읜ㄹ?니니먀핌ㄳ;럼ㅑ</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>56079</th>\n      <td>1</td>\n      <td>;ㅣㅏㅓ</td>\n    </tr>\n    <tr>\n      <th>56080</th>\n      <td>1</td>\n      <td>ㅁㄴㅇㄹ호ㅓㅏㅣ</td>\n    </tr>\n    <tr>\n      <th>56081</th>\n      <td>1</td>\n      <td>ㅂㅈㄷㄱ쇼ㅕㅑ</td>\n    </tr>\n    <tr>\n      <th>56082</th>\n      <td>1</td>\n      <td>ㅋㅌㅊ퓨ㅜ</td>\n    </tr>\n    <tr>\n      <th>56083</th>\n      <td>1</td>\n      <td>ㅁㄴ어뢰ㅏㅓㅎ</td>\n    </tr>\n  </tbody>\n</table>\n<p>56083 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# 랜덤 생성한 아무말 데이터\n",
    "blabladata1 = pd.read_csv('data/blabla.csv', delimiter=',')\n",
    "# 직접 입력한 아무말 데이터\n",
    "# 수집 방법: 인터넷 검색, 주변 사람 인터뷰\n",
    "blabladata2 = pd.read_csv('data/blabladata.csv', delimiter=',')\n",
    "\n",
    "blabla_data = pd.concat([blabladata1, blabladata2], ignore_index=True)\n",
    "del [[blabladata1, blabladata2]] # 메모리 챙겨\n",
    "\n",
    "blabla_data.drop_duplicates(inplace=True)\n",
    "blabla_data"
   ]
  },
  {
   "source": [
    "<li> 아무말 데이터: 56,083개"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data2 = blabla_data['키워드'] # data\n",
    "y_data2 = blabla_data['분류'] # label\n",
    "del [blabla_data] # 메모리 챙겨\n",
    "\n",
    "c = 0\n",
    "\n",
    "step2 = widgets.Label(value=str(c), disabled=True)\n",
    "display(step2)\n",
    "\n",
    "tokenized_blabla = []\n",
    "\n",
    "for i, s in enumerate(X_data2):\n",
    "    tmp = kkma.morphs(s)\n",
    "    tokenized_blabla.append(tmp)\n",
    "    c += 1\n",
    "    step2.value = str(c)\n",
    "\n",
    "print(len(tokenized_blabla), tokenized_blabla[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python365jvsc74a57bd0e7c37ba80a6e54a3d63188245ab5de6a3e0d381993bcb1990a7020536fc2299e",
   "display_name": "Python 3.6.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "e7c37ba80a6e54a3d63188245ab5de6a3e0d381993bcb1990a7020536fc2299e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}